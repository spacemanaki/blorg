#+TITLE: Just LOOK at the humongous type that Hindley-Milner infers for this tiny program!
#+AUTHOR: aki

** Or, check out this one weird trick to make Hindley-Milner blow up! Haskell and ML compilers hate it!

In May I gave a mini-talk at [[http://bangbangcon.com/][!!Con]], and it was loads of fun! The
videos and transcripts of all of the talks are online so you can [[http://bangbangcon.com/recordings.html][watch
them]] if you weren't able to attend and haven't seen them yet.

My talk was on the performance of Hindley-Milner type inference. The
punch line is that the worst case performance is exponential in both
time and space in the size of the input program. When I first learned
about this, I found it both fascinating and baffling since exponential
time is pretty bad!

** Talks are HARD! Ten minutes talks are REALLY HARD!!

This was my first conference talk, and in fact !!Con was the first
conference I've ever attended. I submitted the proposal fully
expecting to be rejected, and also submitted it with [[http://weareallaweso.me/for_speakers/starting-with-nothing.html][very little
material prepared]]. I had worked on an implementation of baby
Hindley-Milner before (no let-polymorphism, which turns out to be
relevant), but all I knew about the performance was that this edge
case exists, and I didn't really understand why.

In fact, after [[https://twitter.com/spacemanaki/status/463496401469833217][many hours or studying]], I still didn't really
understand what was going on. I had planned on trying to overprepare
for the talk, thinking that in 10 minutes the best I could hope for
would be to capture the essence of the edge case in a superficial way,
and rely on a deep well of understanding to back it up and pick out
what was important to highlight. I don't think I ever really managed
to get that deep well even after giving the talk and going over my
notes many weeks later.

I thought it would be helpful (for myself as much as anyone else) if I
wrote up the contents of the talk as a blog post and along the way
fleshed out some of the finer points. I should emphasize that there
are still large gaps in my understanding so what follows is definitely
an incomplete picture and may even be erroneous. On the other hand,
I've spent quite a lot of time thinking about this, and perhaps I can
save someone else a bit of time by capturing what I've found.

** Haskell and ML

All of the examples in this post use Standard ML, but anywhere I say
"ML" you can imagine I'm talking about Haskell, OCaml, F#, or any
other language that uses Hindley-Milner, since at least as far as the
type inference algorithms are concerned, these languages are
similar. In actual fact, the papers that prove the exponential time
result boil things down to a core ML language which is only a little
bit richer than the simply typed lambda calculus.

If you're not familiar with Haskell or ML, this post may still be
interesting but it will be harder going. At the beginning of my talk I
tried to give the audience a crash course on the syntax of the lambda
calculus so that everyone would at least be able to read my slides,
but I'm going to skip that overview here, on the assumption that there
are better resources for learning about ML and the lambda calculus
than a hasty introduction written by me. I've included a couple of
resources for learning ML at the bottom of the post.

** Hindley-Milner

I ran across this edge case in [[http://stackoverflow.com/questions/22060592/very-long-type-inference-sml-trick][a Stack Overflow question (and answer)]],
and although that answer actually sums up everything in this post and
in my talk, I ended up having to research quite a bit more since I
wasn't able to wrap my head around it with just the examples given
there. Most of my understanding I derived from the paper "Deciding ML
typability is complete for deterministic exponential time" published
by Harry G. Mairson in 1990.

The crux of the issue is that there are two features of Hindley-Milner
type systems that work together to ruin the performance of the
algorithm. Both of them relate to `let` expressions, and in fact if
you take `let` away you can do type inference in linear time. That
fact alone is quite interesting to me, since while `let` is an
important feature of ML, it appears to come at quite a high cost!

The two features involved in the pathological case are
let-polymorphism and the ability of `let` to express exponential
function composition. Together, they enable an ML programmer to write
programs which construct expressions whose types are actually
exponential in size. In fact, the main reason that this edge case is
not a problem in practice and that Hindley-Milner is "fast enough" is
that it's extraordinarily rare to have types this large in real
programs!

I'm still shaky on exactly how these two features conspire, and
especially how they result in a worst case exponential
/time/. Unfortunately all of the material on this is either at an
introductory level or at a kind of nuclear-powered or weapons-grade
computability and complexity theory level. So bear with me.

** let-polymorphism

The first feature of Hindley-Milner that I'll discuss is
"let-polymorphism", but before I get to it I should briefly review ML
style polymorphism. You may be familiar with this in Haskell or ML but
if not you might have encountered it in Java, C#, Scala and other
langauges in the form of "generics".

*** parametric polymorphism

Parametric polymorphism is a feature of a type system that enables
code reuse by introducing type variables that can range over any other
type. The simplest example of a polymorphic value is the identity
function (in Standard ML):

#+BEGIN_SRC sml
  fun id x = x
#+END_SRC

The type of this function is `'a -> 'a`, which means that it takes a
value any type, and returns a value of that same type (`->` is the
type constructor for functions). Without parametric polymorphism,
you'd have to write a version of the identity function for every
concrete type: one for integers, booleans, characters, strings,
etc. The same applies to more interesting functions like `map`:

#+BEGIN_SRC sml
  fun map f l = ...
#+END_SRC

The type of `map` is `('a -> 'b) -> 'a list -> 'b list` and it can be
used with lists containing values of any type, and functions mapping
that type to any other type (in ML, type constructors are postfix, so
`'a list` means "list of `'a`", a.k.a. `List<A>` in Java).

*** let vs lambda

There are two kinds of local variables in ML, let-bound variables
which are introduced by `let` and lambda-bound variables which are
arguments in a `lambda` expression, i.e. an anonymous function
expression. (arguments to named functions are considered the same as
lambda-bound variables, since function declarations can be treated as
syntactic sugar.)

If you're coming from a Lisp or a Scheme, which is where I was before
learning ML, then you're probably familiar with the relationship
between `let` and `lambda`. When first being introduced to macros,
`let` is often used as an early example, because you can implement
`let` as a macro, in terms of `lambda` and function application. For
example:

#+BEGIN_SRC sml
  let
     val x = e
  in
     body
  end
#+END_SRC

would be transformed into

#+BEGIN_SRC sml
  (fn x => body) e
#+END_SRC

Both of these creates a local variable named `x`, binds it to the
result of `e`, and evaluates the `body` expression. JavaScript
programmers call this second form an
immediately-invoked-function-expression, and it's a way to introduce
local variables (leveraging the fact that JavaScript only has function
scope).

It turns out that there's a crucial difference with the way let-bound
and lambda-bound variables are typed in Hindley-Milner languages.

Here's an example of a program using `let`:

#+BEGIN_SRC sml
  let
     val id = fn x => x
  in
     (id 3, id true)
  end
#+END_SRC

It introduces a polymorphic identity function, binds it to `id`, and
then calls it with `3` and `true`. This type checks under
Hindley-Milner without any problem.

Now here's the same example if you transformed it as if `let` was a macro:

#+BEGIN_SRC sml
  (fn id => (id 3, id true)) (fn x => x)
#+END_SRC

In this case, the function on the left is being applied to an
anonymous identity function, binding it to `id` and calling it with
`3` and `true` again. This doesn't type check under Hindley-Milner.

The reason that this program doesn't type check but the previous one
does is that lambda-bound variables are not allowed to have
polymorphic values, but let-bound variables are. So in ML, `let` is
more than syntactic sugar, and this feature is called
"let-polymorphism".

(Why can't you just allow polymorphic lambda-bound variables?)

One way that `let-polymorphism` could be implemented in a type checker
is to literally copy and paste the code. In the example above the
compiler would generate an integer and a boolean version of the
identity function and apply them to the right arguments in the
body. Of course, there are other ways to implement it, but I think
that this naive approach hints at the complications that it
introduces.

(What are the other ways of implementing it?)

** exponential function composition

The second feature of ML that conspires to degrade the performance of
Hindley-Milner is something that just sort of falls out from the way
that `let` works. It allows us to concisely express exponential
function composition. In essence, `let` allows us to write a series of
increasingly larger programs which only grow in size linearly, but
consist of composing a function an exponential number of times.

As an example, here's a little ML program that uses `let`:

#+BEGIN_SRC sml
  let val x0 = fn x => x in
     let val x1 = fn y => x0(x0(y)) in
        x1
     end
  end
#+END_SRC

The first `let` binds `x0` to the familiar identity function: `fn x =>
x`. The second `let` binds `x1` to a function that composes `x0`
twice. The body of the these nested `let`s returns `x1` which actually
behaves the same as the identity function. Leaving aside that this
program does't do anything very interesting for a moment, here's
another program with nested `let`s, that's got one extra level of
nesting:

#+BEGIN_SRC sml
  let val x0 = fn x => x in
     let val x1 = fn y => x0(x0(y)) in
        let val x2 = fn y => x1(x1(y)) in
           let val x3 = fn y => x2(x2(y)) in
           x3
           end
        end
     end
  end
#+END_SRC

The extra `let`s bind `x2` to a function that composes `x1` twice, and
then `x3` to a function that composes `x2` twice. Hopefully you can
see where this is going: each time we add an extra nested `let`, we
double the number of times that `x0` is being composed. If you were to
transform the `let` expressions by hand, you'd end up with something
like:

#+BEGIN_SRC sml
  val x0 = fn x => x
  val x1 = fn y => x0(x0(y))
  val x2 = fn y => x0(x0(x0(x0(y))))
  val x3 = fn y => x0(x0(x0(x0(x0(x0(x0(x0(y))))))))
#+END_SRC

All of these functions behave the same way as the identity function,
but it's easy to imagine replacing `x0` with something more
interesting. And using the pattern above, you'd able to write a
program that concisely composes `x0` many times. The important point
is that when this program is changed by adding a single nested `let`,
its size grows linearly, but the number of compositions grows
exponentially (by doubling).

Despite all of this type of `x3` is still just `'a -> 'a` because it's
the same as the plain identity function. In order to get a larger type
out of this program, we need to replace `x0` with something more
interesting.

** pathological case

This function will serve as the basis for the pathological case. It
takes an argument of any type, and returns a pair where the argument
is both the first and second parts of the pair. I've named it `square`
because of its type, which is `'a -> 'a * 'a`; the result type is the
product of argument type and itself.

#+BEGIN_SRC sml
  fun square x = (x, x)
#+END_SRC

The function formed by composing `square` with itself repeatedly
constructs binary trees where the leaves are all the same value. For
instance, `square` applied twice produces:

#+BEGIN_SRC sml
  - square (square 2);
  val it = ((2,2),(2,2)) : (int * int) * (int * int)
#+END_SRC

... which might be better visualized as

#+BEGIN_SRC sml
       ,
      / \
     /   \          
    ,     ,
   / \   / \
  2   2 2   2
#+END_SRC

Applying it again gives another level of nesting:

#+BEGIN_SRC sml
  - square (square (square 3));
  val it = (((3,3),(3,3)),((3,3),(3,3)))
    : ((int * int) * (int * int)) * ((int * int) * (int * int))
             ,
            / \
           /   \
          /     \
         /       \
        /         \
       ,           ,
      / \         / \
     /   \       /   \
    ,     ,     ,     ,
   / \   / \   / \   / \
  3   3 3   3 3   3 3   3
#+END_SRC

Notice that each time we apply `square`, the type of the result
doubles in size. The size of the program grows linearly, while the
type grows exponentially. However, the type has a regular structure
with a lot of repetition. You can represet trees like this in linear
space by taking advantage of this to reduce repetition:

#+BEGIN_SRC sml
  type d1 = int * int
  type d2 = d1 * d1
  type d3 = d2 * d2
  - square (square (square 3)) : d3 ;
  val it = (((3,3),(3,3)),((3,3),(3,3))) : d3
#+END_SRC

In the papers this is referred to in a sort of oblique way that
confused me. For instance: 

"it has a principal type which is of length `Omega(2^(2^cn))` when printed as a string, and has a representation as a directed acyclic graph with Omega(2^cn) nodes" [1]

"types must be represented (and printed out) as directed acyclic
graphs, or /dags/, since the string representation of a type may be
exponentially longer than the given expression." [2]

At first, I didn't understand what /printing/ the type had to do with
anything. It seemed like the sort of low-level implementation details
that wouldn't matter when discussing an algorithm's performance. But
it makes sense if you consider that an ML implementation might print
the complete type without taking shortcuts like the one above.

** *

There are many (in fact, I believe infinitely many) different programs
that exhibit the pathological behavior. But the simplest example I've
come across is not that different from the example programs above. All
we have to do is replace `x0` with a pair constructor:

#+BEGIN_SRC sml
  fun pair x = (x, x)
#+END_SRC

This function takes one argument and returns a pair consisting of that
value in both first and second positions of the pair. It's only a bit
more interesting than the identity function but the type alone tells
us a lot about why it's involved in this pathological case: `'a ->
'a * 'a`. Compare it with the type of the identity: `'a -> 'a`.

The difference between these two is the return type. In the pair
constructor, the return type is the product of the argument type with
itself. The return type is twice the size of argument type. If you
apply this function to a value of any type, the return value will have
a type that is twice as large. If we use `let` to compose the pair
constructor an exponential number of times, we'll get a type that is
exponential in size.

(Do we actually need `let` to construct exponentially large types?)

The actual value we end up assembling is a binary tree (pairs of pairs
of pairs ...) where the leaves can be any value we choose, but that
value will be the same, i.e. every leaf will be the same value, which
is the initial seed argument to the pair constructor. The result of
this is that both the value and its type have repeated, identical
substructures. We can turn this tree into a graph with shared
sub-graphs, and the resulting graph is actually linear in size, no
longer exponential.

This comes up in a couple of places but in the relevant papers but
it's always referred to in this oblique way that confused me,
primarily discussing the difference between printing the type at an
interactive prompt (REPL) versus an internal representation as a graph
inside the type checker. But this makes sense if you think about the
way that the data structure will be printed out, in that since there
isn't an immediately obvious way to indicate shared substructures,
you'd probably end up just printing the whole type including the
repeated bits just for clarity's sake.

The way to get around this, and make it impossible to compactly
represent the type, is to force the type inference algorithm to
generate unique type variables at each node in the tree. The way to do
that is to change the initial value from a monomorphic one to a value
with a polymorphic type. The result is an explosion of type variables, since



** empirical evidence

graphs

different flavors of pathological inputs

** "How to compile Turing machines to ML types"

complexity and computability theory

Mairson's proof technique

** References and further reading

*** Links

http://stackoverflow.com/questions/22060592/very-long-type-inference-sml-trick

http://cs.stackexchange.com/questions/6617/concise-example-of-exponential-cost-of-ml-type-inference

*** Papers

1. "Deciding ML typability is complete for deterministic exponential time" Harry G. Mairson
1990

2. Polymorphic Unification and ML Typing
Paris C. Kanellakis & John C. Mitchell
1989

*** Learning ML

Robert Harper of CMU has written [[http://www.cs.cmu.edu/~rwh/smlbook/book.pdf][a very good and freely available
introductory book]] on Standard ML. I'm also fond of ML for the Working
Programmer, but it's not freely available.

One of the !!Con organizers recently [[http://blog.nullspace.io/beginners-guide-to-ocaml-beginners-guides.html][recently blogged]] about getting
started in OCaml, which would be a better choice than SML for more
practical projects (i.e. projects that are not ML compilers, and maybe
even those that are).
