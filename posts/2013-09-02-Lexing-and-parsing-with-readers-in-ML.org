#+TITLE: Lexing and parsing with readers in ML
#+DATE: <2013-08-31 Sat>
#+AUTHOR: aki
#+EMAIL: aki@utahraptor
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not LOGBOOK) date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t toc:t todo:t |:t
#+CREATOR: Emacs 24.3.1 (Org mode 8.0.3)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

In the [[http://spacemanaki.com/blog/2013/08/31/Polymorphic-streams-in-ML/][last post]] I described the journey I took to "discover" an interesting abstraction for streams in Standard ML. I'm using scare quotes around "discover" because I didn't exactly make a revolutionary contribution there; the documentation for the Basis describes this idea in detail and presents a few examples. But since it surprised me when I learned about it, I thought it would be interesting to other people. Assuming it is, I thought it might be helpful to walk through the small example I've been using as a proof of concept while experimenting with this idea.

The example is a lexer and a parser for /s-expressions/, in other words a subset of Scheme or Lisp code. I chose this because it's a very small language that's easy to parse with recursive descent, but it's also complicated enough that the code is illustrative.

## reader

Recall that a reader is a peek function over a stream; it takes a stream and returns an element and new stream (wrapped in `'a option`). Concretely, the type of a reader, as defined in [[http://www.standardml.org/Basis/string-cvt.html][StringCvt]] is:

#+BEGIN_SRC sml
  type ('a,'b) reader = 'b -> ('a * 'b) option
#+END_SRC

#+RESULTS:
: type ('a,'b) reader = 'b -> ('a * 'b) option

## tokens

Our lexer will be a function that operates on readers. Its argument is a character reader and it will return a token reader. The argument produces characters from some type of stream, and the returned reader will produce tokens from the same type of stream:

#+NAME: tokenize_type
#+BEGIN_SRC sml
  val tokenize : (char, 'a) StringCvt.reader -> (token, 'a) StringCvt.reader
#+END_SRC

Tokens are the words that make up our language, in this case the language of  s-expressions. This could be a component in an actual Lisp interpreter or compiler, and we would want to be able to report error messages to the user with line and column number pointing them to the source code that caused the error. Because of that, we will need to carry this information forward from the character stream to the token stream and beyond. The type of this can be a simple record:

#+BEGIN_SRC sml
  type pos = {line: int, col: int}
#+END_SRC

There are only four kinds of tokens in the little s-expression language: left (opening) parentheses, right (closing) parentheses, atoms, and a period (for dotted tails or "improper lists").

#+NAME: token_datatype
#+BEGIN_SRC sml
  datatype token = LParen of pos | RParen of pos | Atom of string * pos | Dot of pos
#+END_SRC

Three of the tokens are single characters, while atoms are the only token longer than one character. The function `tokenize` will need to peek one character ahead in the stream to determine what to do. If that character is one of the single character tokens, all it needs to do is return that token along with the rest of the stream. If the character is some other non-whitespace character, the lexer should read ahead until seeing some character not part of the atom, i.e. one of the other tokens or whitespace. Finally, it should just skip over the whitespace between tokens.

## whitespace

Stripping whitespace from streams, with readers, is something that nearly every character and string processing program will want to do, so there's a function provided in the StringCvt module for just this purpose: `skipWS`. It was helpful for me to understand how to implement this myself, just to get a better intuition for streams and readers:

#+BEGIN_SRC sml
  fun skipWS rdr s = StringCvt.dropl Char.isSpace rdr s
#+END_SRC

Of course, this implementation is cheating a bit, since it's using `dropl`. Here's an implementation that uses nothing but the reader:

#+BEGIN_SRC sml
  fun skipWS rdr s =
      case rdr s of
          NONE => s
        | SOME (x, s') =>
          if Char.isSpace x then
             skipWS rdr s'
          else s'
#+END_SRC

(The predicate `Char.isSpace` does what it says on the tin, and isn't interesting enough to re-implement here)

## tokenize

With this under our belt, we can move on to writing the lexer proper:

#+NAME: tokenize_impl
#+BEGIN_SRC sml
  fun tokenize rdr =
      fn s =>
         case rdr (StringCvt.skipWS rdr s) of
             NONE => NONE
           | SOME (#".", s') => SOME (Dot, s')
           | SOME (#"(", s') => SOME (LParen, s')
           | SOME (#")", s') => SOME (RParen, s')
           | SOME (_, s') =>
             case getAtom rdr (StringCvt.skipWS rdr s) of
                 NONE => NONE
               | SOME (atom, s') => SOME (Atom atom, s')
  
#+END_SRC

Remember that this function takes a reader, and returns another reader. Since readers are functions of streams, the body of this function is an anonymous function of one argument (`fn s =>` on line 5).

The first thing we do is apply the skip whitespace function from StringCvt to the stream, then apply the reader (`rdr (StringCvt.skipWS rdr s)` on line 3). This gives us either `NONE`, indicating the stream is empty, or `SOME` with the peeked character and the rest of the stream. Depending on the character, we either return a single character token or call a helper function named `getAtom`, which takes a reader and a character stream, and returns the next atom in the stream as a string, and the rest of the character stream:

#+NAME: getAtom_impl
#+BEGIN_SRC sml
  fun getAtom rdr s =
      let
         fun return [] _ = NONE
           | return acc s = SOME (String.implode (rev acc), s)
  
         fun getAtom' acc s =
             case rdr s of
                  NONE => return acc s
                | SOME (#"(", rest) => return acc s
                | SOME (#")", rest) => return acc s
                | SOME (x, rest) => if Char.isSpace x then
                                       return acc s
                                    else getAtom' (x :: acc) rest
      in
         getAtom' [] s
      end
#+END_SRC

The function `getAtom` uses a similar pattern, reading characters from the stream until it sees either a parentheses or a space. Together, these function complete the lexical analyzer. Using a little helper reader function `consume` that accumulates a stream into a list, we can test out the lexer pretty easily:

#+BEGIN_SRC sml
  val [LParen, Atom "foo", RParen] = consume (tokenize string) "(foo)"
#+END_SRC

#+BEGIN_SRC sml :exports none :noweb yes :tangle yes
  signature LEXER =
  sig
     <<token_datatype>>
     <<tokenize_type>>
  end
  structure Lexer : LEXER =
  struct
     <<token_datatype>>
     (*
      ,* Given a char reader and stream, try to extract a Scheme atom
      ,* (string) from the stream, and return it with the rest of the stream
      ,*)
     <<getAtom_impl>>
     (*
      ,* Given a char reader, produce a token reader
      ,*)
     <<tokenize_impl>>
  end
#+END_SRC

#+RESULTS:
: [autoloading]
: [library $SMLNJ-BASIS/basis.cm is stable]
: [autoloading done]
: signature LEXER =
:   sig
:     datatype token = Atom of string | Dot | LParen | RParen
:     val tokenize : (char,'a) reader -> (token,'a) reader
:   end
: structure Lexer : LEXER

## parsing

